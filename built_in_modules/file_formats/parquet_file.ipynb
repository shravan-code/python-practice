{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Parquet File Format (Apache Parquet)\n",
    "\n",
    "Apache Parquet is an open-source, columnar storage file format optimized for analytical workloads (OLAP). It is widely used across big data and cloud ecosystemsâ€”Spark, Hive, Presto/Trino, Athena, BigQuery, Snowflake, and data lakes on S3, ADLS, and GCS."
   ],
   "id": "846e3e7623f9fe4a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Columnar Storage\n",
    "\n",
    "Unlike row-based formats (CSV, JSON, Avro-row), Parquet stores each column contiguously on disk.\n",
    "\n",
    "**Implications**\n",
    "\n",
    "1. Reads only the columns required by a query (I/O reduction).\n",
    "2. Achieves higher compression due to homogeneous data per column.\n",
    "3. Enables vectorized execution and predicate pushdown."
   ],
   "id": "6e5842054d562be8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Internal Structure\n",
    "\n",
    "A Parquet file is organized hierarchically:\n",
    "\n",
    "    * File\n",
    "        * Row Groups (horizontal partitions)\n",
    "            * Column Chunks (one per column)\n",
    "                 * Pages\n",
    "                    * Data Pages\n",
    "                        * Dictionary Pages (optional)\n",
    "\n",
    "**Metadata**\n",
    "\n",
    "* Stored in the file footer.\n",
    "* Contains schema, statistics (min/max/null count), encodings, and offsets.\n",
    "* Enables predicate pushdown and column pruning without scanning data."
   ],
   "id": "6a0e2befdbeb5ad7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Compression & Encoding\n",
    "\n",
    "Parquet separates encoding from compression.\n",
    "\n",
    "**Encodings**\n",
    "\n",
    "* Plain\n",
    "* Dictionary Encoding\n",
    "* Run-Length Encoding (RLE)\n",
    "* Delta Encoding (for sorted/numeric data)\n",
    "* Bit Packing\n",
    "\n",
    "**Compression Codecs**\n",
    "\n",
    "* Snappy (default in Spark; fast)\n",
    "* Gzip (higher compression, slower\n",
    "* ZSTD (excellent balance; increasingly popular)\n",
    "* Brotli, LZO (environment-dependent)"
   ],
   "id": "607badac0ce8fa91"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Schema Support\n",
    "\n",
    "* Strongly typed schema (INT, BIGINT, FLOAT, DOUBLE, BOOLEAN, BYTE_ARRAY, etc.)\n",
    "* Nested data support (structs, arrays, maps)\n",
    "* Schema evolution (add columns safely; dropping requires care)\n"
   ],
   "id": "fe7e8f2d8f1ea142"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Performance Characteristics\n",
    "\n",
    "**Strengths**\n",
    "\n",
    "* Excellent for aggregates, filters, scans, joins.\n",
    "* Efficient for large-scale analytics.\n",
    "* Works well with partitioning (e.g., date=YYYY-MM-DD).\n",
    "\n",
    "**Limitations**\n",
    "\n",
    "* Not suitable for frequent small updates or deletes.\n",
    "* Row-level mutations are expensive (rewrite required).\n",
    "* Poor fit for transactional OLTP without a table format layer."
   ],
   "id": "6a747875f7ed3601"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "| Aspect      | Parquet   | CSV  | JSON    | Avro      |\n",
    "| ----------- | --------- | ---- | ------- | --------- |\n",
    "| Storage     | Columnar  | Row  | Row     | Row       |\n",
    "| Compression | Excellent | Poor | Poor    | Moderate  |\n",
    "| Schema      | Yes       | No   | Weak    | Yes       |\n",
    "| Analytics   | Excellent | Poor | Poor    | Good      |\n",
    "| Streaming   | No        | No   | Limited | Excellent |\n"
   ],
   "id": "bbac8edad82962c8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Rule of thumb**\n",
    "\n",
    "`Analytics / Data Lake`:  **Parquet**\n",
    "\n",
    "`Streaming / Messaging`:  **Avro**\n",
    "\n",
    "`Interchange / Debugging`:  **JSON**\n",
    "\n",
    "`Human-readable export`:  **CSV**"
   ],
   "id": "4336c3844a55b219"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-19T17:05:44.840481300Z",
     "start_time": "2025-12-19T17:05:38.544402300Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T17:05:44.930687900Z",
     "start_time": "2025-12-19T17:05:44.850502300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rows = [\n",
    "    {\"id\": 1, \"name\": \"Shravan\", \"age\": 28},\n",
    "    {\"id\": 2, \"name\": \"Hanvika\", \"age\": 25}\n",
    "]"
   ],
   "id": "ef87950bd39ccd01",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T17:05:45.039977400Z",
     "start_time": "2025-12-19T17:05:44.935929900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.DataFrame(rows)\n",
    "df"
   ],
   "id": "803e570d7d75c801",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   id     name  age\n",
       "0   1  Shravan   28\n",
       "1   2  Hanvika   25"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Shravan</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Hanvika</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T17:05:47.346132200Z",
     "start_time": "2025-12-19T17:05:45.154130900Z"
    }
   },
   "cell_type": "code",
   "source": "df.to_parquet('mydata.parquet', compression='snappy')",
   "id": "f8d866f77bdfa667",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T17:06:09.371853700Z",
     "start_time": "2025-12-19T17:06:09.263376500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "os.listdir()"
   ],
   "id": "2a829bfc4091b632",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['avro_file.ipynb',\n",
       " 'config_file.ipynb',\n",
       " 'csv_file.ipynb',\n",
       " 'json_file.ipynb',\n",
       " 'mydata.parquet',\n",
       " 'parquet_file.ipynb',\n",
       " 'toml_file.ipynb']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T17:07:54.287739100Z",
     "start_time": "2025-12-19T17:07:54.206664800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df2 = pd.read_parquet('mydata.parquet')\n",
    "df2"
   ],
   "id": "b2502aea2f44ed8a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   id     name  age\n",
       "0   1  Shravan   28\n",
       "1   2  Hanvika   25"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Shravan</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Hanvika</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T17:09:29.343045800Z",
     "start_time": "2025-12-19T17:09:29.278130500Z"
    }
   },
   "cell_type": "code",
   "source": "os.unlink('mydata.parquet')",
   "id": "e6f3c341e1dea920",
   "outputs": [],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
